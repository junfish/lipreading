{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Lip-Reading-Model-for-GRID.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "HKLEbTNe8Lwo",
        "_GMnz-Xnnlt5"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuLEOyCM6q6w"
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!pwd\n",
        "os.chdir('gdrive/My Drive/CSE_498_CV/Lip_Reading_Project/LipReading/')\n",
        "!pwd\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhnJuif85RMQ"
      },
      "source": [
        "# Set up environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0J8aFi0q5Qwt"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!pip install q keras==2.0.2\n",
        "!pip install sk-video\n",
        "!pip install h5py==2.6.0\n",
        "!pip install scipy==1.1.0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKLEbTNe8Lwo"
      },
      "source": [
        "# videos.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdKcVJ0m8Non",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ceb50dc-3dd4-4b40-bdbe-421336900705"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from keras import backend as K\n",
        "from scipy import ndimage\n",
        "from scipy.misc import imresize\n",
        "import skvideo.io\n",
        "import dlib\n",
        "from lipnet.lipreading.aligns import Align\n",
        "\n",
        "class VideoAugmenter(object):\n",
        "    @staticmethod\n",
        "    def split_words(video, align):\n",
        "        video_aligns = []\n",
        "        for sub in align.align:\n",
        "            # Create new video\n",
        "            _video = Video(video.vtype, video.face_predictor_path)\n",
        "            _video.face = video.face[sub[0]:sub[1]]\n",
        "            _video.mouth = video.mouth[sub[0]:sub[1]]\n",
        "            _video.set_data(_video.mouth)\n",
        "            # Create new align\n",
        "            _align = Align(align.absolute_max_string_len, align.label_func).from_array([(0, sub[1]-sub[0], sub[2])])\n",
        "            # Append\n",
        "            video_aligns.append((_video, _align))\n",
        "        return video_aligns\n",
        "\n",
        "    @staticmethod\n",
        "    def merge(video_aligns):\n",
        "        vsample = video_aligns[0][0]\n",
        "        asample = video_aligns[0][1]\n",
        "        video = Video(vsample.vtype, vsample.face_predictor_path)\n",
        "        video.face = np.ones((0, vsample.face.shape[1], vsample.face.shape[2], vsample.face.shape[3]), dtype=np.uint8)\n",
        "        video.mouth = np.ones((0, vsample.mouth.shape[1], vsample.mouth.shape[2], vsample.mouth.shape[3]), dtype=np.uint8)\n",
        "        align = []\n",
        "        inc = 0\n",
        "        for _video, _align in video_aligns:\n",
        "            video.face = np.concatenate((video.face, _video.face), 0)\n",
        "            video.mouth = np.concatenate((video.mouth, _video.mouth), 0)\n",
        "            for sub in _align.align:\n",
        "                _sub = (sub[0]+inc, sub[1]+inc, sub[2])\n",
        "                align.append(_sub)\n",
        "            inc = align[-1][1]\n",
        "        video.set_data(video.mouth)\n",
        "        align = Align(asample.absolute_max_string_len, asample.label_func).from_array(align)\n",
        "        return (video, align)\n",
        "\n",
        "    @staticmethod\n",
        "    def pick_subsentence(video, align, length):\n",
        "        split = VideoAugmenter.split_words(video, align)\n",
        "        start = np.random.randint(0, align.word_length - length)\n",
        "        return VideoAugmenter.merge(split[start:start+length])\n",
        "\n",
        "    @staticmethod\n",
        "    def pick_word(video, align):\n",
        "        video_aligns = np.array(VideoAugmenter.split_words(video, align))\n",
        "        return video_aligns[np.random.randint(video_aligns.shape[0], size=2), :][0]\n",
        "\n",
        "    @staticmethod\n",
        "    def horizontal_flip(video):\n",
        "        _video = Video(video.vtype, video.face_predictor_path)\n",
        "        _video.face = np.flip(video.face, 2)\n",
        "        _video.mouth = np.flip(video.mouth, 2)\n",
        "        _video.set_data(_video.mouth)\n",
        "        return _video\n",
        "\n",
        "    @staticmethod\n",
        "    def temporal_jitter(video, probability):\n",
        "        changes = [] # [(frame_i, type=del/dup)]\n",
        "        t = video.length\n",
        "        for i in range(t):\n",
        "            if np.random.ranf() <= probability/2:\n",
        "                changes.append((i, 'del'))\n",
        "            if probability/2 < np.random.ranf() <= probability:\n",
        "                changes.append((i, 'dup'))\n",
        "        _face = np.copy(video.face)\n",
        "        _mouth = np.copy(video.mouth)\n",
        "        j = 0\n",
        "        for change in changes:\n",
        "            _change = change[0] + j\n",
        "            if change[1] == 'dup':\n",
        "                _face = np.insert(_face, _change, _face[_change], 0)\n",
        "                _mouth = np.insert(_mouth, _change, _mouth[_change], 0)\n",
        "                j = j + 1\n",
        "            else:\n",
        "                _face = np.delete(_face, _change, 0)\n",
        "                _mouth = np.delete(_mouth, _change, 0)\n",
        "                j = j - 1\n",
        "        _video = Video(video.vtype, video.face_predictor_path)\n",
        "        _video.face = _face\n",
        "        _video.mouth = _mouth\n",
        "        _video.set_data(_video.mouth)\n",
        "        return _video\n",
        "\n",
        "    @staticmethod\n",
        "    def pad(video, length):\n",
        "        pad_length = max(length - video.length, 0)\n",
        "        video_length = min(length, video.length)\n",
        "        face_padding = np.ones((pad_length, video.face.shape[1], video.face.shape[2], video.face.shape[3]), dtype=np.uint8) * 0\n",
        "        mouth_padding = np.ones((pad_length, video.mouth.shape[1], video.mouth.shape[2], video.mouth.shape[3]), dtype=np.uint8) * 0\n",
        "        _video = Video(video.vtype, video.face_predictor_path)\n",
        "        _video.face = np.concatenate((video.face[0:video_length], face_padding), 0)\n",
        "        _video.mouth = np.concatenate((video.mouth[0:video_length], mouth_padding), 0)\n",
        "        _video.set_data(_video.mouth)\n",
        "        return _video\n",
        "\n",
        "\n",
        "class Video(object):\n",
        "    def __init__(self, vtype='mouth', face_predictor_path=None):\n",
        "        if vtype == 'face' and face_predictor_path is None:\n",
        "            raise AttributeError('Face video need to be accompanied with face predictor')\n",
        "        self.face_predictor_path = face_predictor_path\n",
        "        self.vtype = vtype\n",
        "\n",
        "    def from_frames(self, path):\n",
        "        frames_path = sorted([os.path.join(path, x) for x in os.listdir(path)])\n",
        "        frames = [ndimage.imread(frame_path) for frame_path in frames_path]\n",
        "        self.handle_type(frames)\n",
        "        return self\n",
        "\n",
        "    def from_video(self, path):\n",
        "        frames = self.get_video_frames(path)\n",
        "        self.handle_type(frames)\n",
        "        return self\n",
        "\n",
        "    def from_array(self, frames):\n",
        "        self.handle_type(frames)\n",
        "        return self\n",
        "\n",
        "    def handle_type(self, frames):\n",
        "        if self.vtype == 'mouth':\n",
        "            self.process_frames_mouth(frames)\n",
        "        elif self.vtype == 'face':\n",
        "            self.process_frames_face(frames)\n",
        "        else:\n",
        "            raise Exception('Video type not found')\n",
        "\n",
        "    def process_frames_face(self, frames):\n",
        "        detector = dlib.get_frontal_face_detector()\n",
        "        predictor = dlib.shape_predictor(self.face_predictor_path)\n",
        "        mouth_frames = self.get_frames_mouth(detector, predictor, frames)\n",
        "        self.face = np.array(frames)\n",
        "        self.mouth = np.array(mouth_frames)\n",
        "        self.set_data(mouth_frames)\n",
        "\n",
        "    def process_frames_mouth(self, frames):\n",
        "        self.face = np.array(frames)\n",
        "        self.mouth = np.array(frames)\n",
        "        self.set_data(frames)\n",
        "\n",
        "    def get_frames_mouth(self, detector, predictor, frames):\n",
        "        MOUTH_WIDTH = 100\n",
        "        MOUTH_HEIGHT = 50\n",
        "        HORIZONTAL_PAD = 0.19\n",
        "        normalize_ratio = None\n",
        "        mouth_frames = []\n",
        "        for frame in frames:\n",
        "            dets = detector(frame, 1)\n",
        "            shape = None\n",
        "            for k, d in enumerate(dets):\n",
        "                shape = predictor(frame, d)\n",
        "                i = -1\n",
        "            if shape is None: # Detector doesn't detect face, just return as is\n",
        "                return frames\n",
        "            mouth_points = []\n",
        "            for part in shape.parts():\n",
        "                i += 1\n",
        "                if i < 48: # Only take mouth region\n",
        "                    continue\n",
        "                mouth_points.append((part.x,part.y))\n",
        "            np_mouth_points = np.array(mouth_points)\n",
        "\n",
        "            mouth_centroid = np.mean(np_mouth_points[:, -2:], axis=0)\n",
        "\n",
        "            if normalize_ratio is None:\n",
        "                mouth_left = np.min(np_mouth_points[:, :-1]) * (1.0 - HORIZONTAL_PAD)\n",
        "                mouth_right = np.max(np_mouth_points[:, :-1]) * (1.0 + HORIZONTAL_PAD)\n",
        "\n",
        "                normalize_ratio = MOUTH_WIDTH / float(mouth_right - mouth_left)\n",
        "\n",
        "            new_img_shape = (int(frame.shape[0] * normalize_ratio), int(frame.shape[1] * normalize_ratio))\n",
        "            resized_img = imresize(frame, new_img_shape)\n",
        "\n",
        "            mouth_centroid_norm = mouth_centroid * normalize_ratio\n",
        "\n",
        "            mouth_l = int(mouth_centroid_norm[0] - MOUTH_WIDTH / 2)\n",
        "            mouth_r = int(mouth_centroid_norm[0] + MOUTH_WIDTH / 2)\n",
        "            mouth_t = int(mouth_centroid_norm[1] - MOUTH_HEIGHT / 2)\n",
        "            mouth_b = int(mouth_centroid_norm[1] + MOUTH_HEIGHT / 2)\n",
        "\n",
        "            mouth_crop_image = resized_img[mouth_t:mouth_b, mouth_l:mouth_r]\n",
        "\n",
        "            mouth_frames.append(mouth_crop_image)\n",
        "        return mouth_frames\n",
        "\n",
        "    def get_video_frames(self, path):\n",
        "        videogen = skvideo.io.vreader(path)\n",
        "        frames = np.array([frame for frame in videogen])\n",
        "        return frames\n",
        "\n",
        "    def set_data(self, frames):\n",
        "        data_frames = []\n",
        "        for frame in frames:\n",
        "            frame = frame.swapaxes(0,1) # swap width and height to form format W x H x C\n",
        "            if len(frame.shape) < 3:\n",
        "                frame = np.array([frame]).swapaxes(0,2).swapaxes(0,1) # Add grayscale channel\n",
        "            data_frames.append(frame)\n",
        "        frames_n = len(data_frames)\n",
        "        data_frames = np.array(data_frames) # T x W x H x C\n",
        "        if K.image_data_format() == 'channels_first':\n",
        "            data_frames = np.rollaxis(data_frames, 3) # C x T x W x H\n",
        "        self.data = data_frames\n",
        "        self.length = frames_n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GMnz-Xnnlt5"
      },
      "source": [
        "# Model 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVFsFqHqnm39"
      },
      "source": [
        "from keras.layers.convolutional import Conv3D, ZeroPadding3D\n",
        "from keras.layers.pooling import MaxPooling3D\n",
        "from keras.layers.core import Dense, Activation, SpatialDropout3D, Flatten\n",
        "from keras.layers.wrappers import Bidirectional, TimeDistributed\n",
        "from keras.layers.recurrent import GRU\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from lipnet.core.layers import CTC\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "class LipNet(object):\n",
        "    def __init__(self, img_c=3, img_w=100, img_h=50, frames_n=75, absolute_max_string_len=32, output_size=28):\n",
        "        self.img_c = img_c\n",
        "        self.img_w = img_w\n",
        "        self.img_h = img_h\n",
        "        self.frames_n = frames_n\n",
        "        self.absolute_max_string_len = absolute_max_string_len\n",
        "        self.output_size = output_size\n",
        "        self.build()\n",
        "\n",
        "    def build(self):\n",
        "        if K.image_data_format() == 'channels_first':\n",
        "            input_shape = (self.img_c, self.frames_n, self.img_w, self.img_h)\n",
        "        else:\n",
        "            input_shape = (self.frames_n, self.img_w, self.img_h, self.img_c)\n",
        "\n",
        "        self.input_data = Input(name='the_input', shape=input_shape, dtype='float32')\n",
        "\n",
        "        self.zero1 = ZeroPadding3D(padding=(1, 2, 2), name='zero1')(self.input_data)\n",
        "        self.conv1 = Conv3D(32, (3, 5, 5), strides=(1, 2, 2), kernel_initializer='he_normal', name='conv1')(self.zero1)\n",
        "        self.batc1 = BatchNormalization(name='batc1')(self.conv1)\n",
        "        self.actv1 = Activation('relu', name='actv1')(self.batc1)\n",
        "        self.drop1 = SpatialDropout3D(0.5)(self.actv1)\n",
        "        self.maxp1 = MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2), name='max1')(self.drop1)\n",
        "\n",
        "        self.zero2 = ZeroPadding3D(padding=(1, 2, 2), name='zero2')(self.maxp1)\n",
        "        self.conv2 = Conv3D(64, (3, 5, 5), strides=(1, 1, 1), kernel_initializer='he_normal', name='conv2')(self.zero2)\n",
        "        self.batc2 = BatchNormalization(name='batc2')(self.conv2)\n",
        "        self.actv2 = Activation('relu', name='actv2')(self.batc2)\n",
        "        self.drop2 = SpatialDropout3D(0.5)(self.actv2)\n",
        "        self.maxp2 = MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2), name='max2')(self.drop2)\n",
        "\n",
        "        self.zero3 = ZeroPadding3D(padding=(1, 1, 1), name='zero3')(self.maxp2)\n",
        "        self.conv3 = Conv3D(96, (3, 3, 3), strides=(1, 1, 1), kernel_initializer='he_normal', name='conv3')(self.zero3)\n",
        "        self.batc3 = BatchNormalization(name='batc3')(self.conv3)\n",
        "        self.actv3 = Activation('relu', name='actv3')(self.batc3)\n",
        "        self.drop3 = SpatialDropout3D(0.5)(self.actv3)\n",
        "        self.maxp3 = MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2), name='max3')(self.drop3)\n",
        "\n",
        "        self.resh1 = TimeDistributed(Flatten())(self.maxp3)\n",
        "\n",
        "        self.gru_1 = Bidirectional(GRU(256, return_sequences=True, kernel_initializer='Orthogonal', name='gru1'), merge_mode='concat')(self.resh1)\n",
        "        self.gru_2 = Bidirectional(GRU(256, return_sequences=True, kernel_initializer='Orthogonal', name='gru2'), merge_mode='concat')(self.gru_1)\n",
        "\n",
        "        # transforms RNN output to character activations:\n",
        "        self.dense1 = Dense(self.output_size, kernel_initializer='he_normal', name='dense1')(self.gru_2)\n",
        "\n",
        "        self.y_pred = Activation('softmax', name='softmax')(self.dense1)\n",
        "\n",
        "        self.labels = Input(name='the_labels', shape=[self.absolute_max_string_len], dtype='float32')\n",
        "        self.input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
        "        self.label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
        "\n",
        "        self.loss_out = CTC('ctc', [self.y_pred, self.labels, self.input_length, self.label_length])\n",
        "\n",
        "        self.model = Model(inputs=[self.input_data, self.labels, self.input_length, self.label_length], outputs=self.loss_out)\n",
        "\n",
        "    def summary(self):\n",
        "        Model(inputs=self.input_data, outputs=self.y_pred).summary()\n",
        "\n",
        "    def predict(self, input_batch):\n",
        "        return self.test_function([input_batch, 0])[0]  # the first 0 indicates test\n",
        "\n",
        "    @property\n",
        "    def test_function(self):\n",
        "        # captures output of softmax so we can decode the output during visualization\n",
        "        return K.function([self.input_data, K.learning_phase()], [self.y_pred, K.learning_phase()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keFh8iLV7ic5"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFizfo3W74k0"
      },
      "source": [
        "# Utilize LipNet\n",
        "from lipnet.lipreading.visualization import show_video_subtitle\n",
        "from lipnet.core.decoders import Decoder\n",
        "from lipnet.lipreading.helpers import labels_to_text\n",
        "from lipnet.utils.spell import Spell\n",
        "from keras.optimizers import Adam\n",
        "from keras import backend as K\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "\n",
        "def my_func(arg):\n",
        "  arg = tf.convert_to_tensor(arg, dtype=tf.float32)\n",
        "  return arg\n",
        "\n",
        "\n",
        "np.random.seed(55)\n",
        "CURRENT_PATH = '/content/gdrive/My Drive/CSE_498_CV/Lip_Reading_Project/LipReading/'\n",
        "FACE_PREDICTOR_PATH = os.path.join(CURRENT_PATH,'common','predictors','shape_predictor_68_face_landmarks.dat')\n",
        "\n",
        "PREDICT_GREEDY      = False\n",
        "PREDICT_BEAM_WIDTH  = 200\n",
        "PREDICT_DICTIONARY  = os.path.join(CURRENT_PATH,'common','dictionaries','grid.txt')\n",
        "\n",
        "WEIGHT_PATH = os.path.join(CURRENT_PATH,'evaluation','models','My_trained_model.h5')\n",
        "VIDEO_PATH = os.path.join(CURRENT_PATH,'evaluation','samples','My_video3.mp4')\n",
        "\n",
        "print(WEIGHT_PATH)\n",
        "\n",
        "\n",
        "def predict(weight_path, video_path, absolute_max_string_len=32, output_size=28):\n",
        "    print (\"\\nLoading data from disk...\")\n",
        "    video = Video(vtype='face', face_predictor_path=FACE_PREDICTOR_PATH)\n",
        "    if os.path.isfile(video_path):\n",
        "        print (\"Data loaded from video.\\n\")\n",
        "        video.from_video(video_path)\n",
        "        print (\"Finish loading data.\\n\")\n",
        "    else:\n",
        "        print (\"Data loaded from frame.\\n\")\n",
        "        video.from_frames(video_path)\n",
        "        print (\"Finish loading data.\\n\")\n",
        "\n",
        "    if K.image_data_format() == 'channels_first':\n",
        "        img_c, frames_n, img_w, img_h = video.data.shape\n",
        "    else:\n",
        "        frames_n, img_w, img_h, img_c = video.data.shape\n",
        "\n",
        "\n",
        "    lipnet = LipNet(img_c=img_c, img_w=img_w, img_h=img_h, frames_n=frames_n, absolute_max_string_len=absolute_max_string_len, output_size=output_size)\n",
        "\n",
        "    adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
        "\n",
        "    lipnet.model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=adam)\n",
        "    lipnet.model.load_weights(weight_path)\n",
        "\n",
        "    spell = Spell(path=PREDICT_DICTIONARY)\n",
        "    decoder = Decoder(greedy=PREDICT_GREEDY, beam_width=PREDICT_BEAM_WIDTH,\n",
        "                      postprocessors=[labels_to_text, spell.sentence])\n",
        "\n",
        "    X_data       = np.array([video.data]).astype(np.float32) / 255\n",
        "    input_length = np.array([len(video.data)])\n",
        "\n",
        "    y_pred = lipnet.predict(X_data)\n",
        "    result  = decoder.decode(y_pred, input_length)[0]\n",
        "\n",
        "    return (X_data, video, result)\n",
        "\n",
        "\n",
        "X_data, video, predicted_result = predict(WEIGHT_PATH, VIDEO_PATH)    \n",
        "X_data = X_data[0,:,:,:,:]\n",
        "\n",
        "print(predicted_result)\n",
        "\n",
        "if video is not None:\n",
        "        show_video_subtitle(video.face, predicted_result)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}